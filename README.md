# 🧬 갑상선 BRAF 변이 예측 모델 (Thyroid Mutation Prediction Model)

본 프로젝트는 **갑상선암 병리 슬라이드(WSI)** 이미지를 기반으로  
딥러닝을 활용하여 **BRAF 변이 여부를 예측**하는 인공지능 모델을 개발하기 위한 연구이자 디지털 병리 기반 정밀의료 기술 개발의 일환입니다.

🔍 연구 배경 및 필요성

BRAF V600E 변이는 갑상선 유두암(Papillary Thyroid Carcinoma, PTC)의 약 80%에서 발견되는 주요 유전적 변화로,
예후, 재발 위험도, 방사성 요오드 치료 반응성 등에 큰 영향을 미칩니다.

그러나 유전자 검사(PCR, NGS 등)는 비용, 시료 제한, 검사 인프라 등의 문제로
모든 환자에게 적용하기 어려운 한계가 있습니다.

따라서 조직학적 형태(H&E 슬라이드)만으로 변이 여부를 추정할 수 있는 AI 모델은
저비용·비침습적 진단을 가능하게 하는 혁신적인 대안이 될 수 있습니다.


| 구분        | 병리의사                     | AI 모델                             |
| --------- | ------------------------ | --------------------------------- |
| **입력 정보** | H&E 병리 슬라이드              | 동일 (WSI 타일)                       |
| **관찰 단위** | 세포핵, 구조, 색조              | 수백~수천 타일의 픽셀 패턴                   |
| **판단 기준** | 경험적 특징 (ex. 유두 구조, 핵 모양) | 통계적 특징 벡터                         |
| **한계**    | 사람 눈으로 보기엔 너무 미세한 차이     | 딥러닝이 고차원 특징에서 구분 가능               |
| **결과**    | “BRAF 변이가 의심된다”          | “이 슬라이드의 BRAF 확률 = 0.92” (정량화 가능) |


🎯 기대 효과

비침습적 유전자 예측: 조직 슬라이드만으로 변이 상태를 판별 → PCR/NGS 검사 보조 가능

검사 효율성 향상: 병리진단 과정 자동화 및 판독시간 단축

정밀의료 기반 강화: 예후 예측 및 맞춤 치료 방안 수립에 기여

확장성: RAS, RET/NTRK 등 다른 변이 탐지로 모델 확장 가능


---

## 🚀 프로젝트 개요

| 단계                                          | 설명                                                                        |
| ------------------------------------------- | ------------------------------------------------------------------------- |
| **1️⃣ 병리 이미지를 입력**                          | H&E 염색된 조직 슬라이드(WSI)를 AI가 입력으로 받습니다.                                      |
| **2️⃣ 타일 분할**                               | 슬라이드를 512×512 같은 작은 타일 단위로 나눕니다.                                          |
| **3️⃣ 특징 추출 (Feature Extraction)**          | CNN, ViT, CLIP, UNI 등 딥러닝 모델이 타일에서 세포 모양·핵의 크기·조직 구조 등을 벡터(숫자 특징)로 변환합니다. |
| **4️⃣ 학습 (Supervised / Weakly supervised)** | 각 슬라이드가 “BRAF 변이 있음 / 없음”으로 라벨되어 있으면, AI는 그 차이를 만드는 **시각적 패턴**을 학습합니다.    |
| **5️⃣ 예측 및 Heatmap 시각화**                    | AI가 “이 부위가 BRAF mutation과 관련 있어 보인다”고 판단한 영역을 Heatmap으로 표시합니다.            |


이 저장소는 **UNI2-h 비전 트랜스포머(Transformer)** 를 활용한  
WSI(Whole Slide Image) 임베딩 파이프라인과,  
**ABMIL(Attention-based Multiple Instance Learning)** 기반 BRAF 변이 예측 모델의  
전체 전처리 및 학습 코드를 포함합니다.

---

## 📊 데이터 구성

| 구분 | 설명 | 수량 |
|------|------|------|
| Meta (BRAF+) | BRAF 변이 양성 병리 슬라이드 | 약 **4,000장 중 2,000장 선택** |
| Non-meta (BRAF−) | 변이 음성 슬라이드 | **862장 전수 사용** |
| 타일 크기 | 512×512 PNG 패치 | 슬라이드당 평균 약 20,000개 |
| 임베딩 벡터 | 1타일당 1536차원 | UNI2-h 기반 feature vector |


---

## ⚙️ 임베딩 파이프라인

### 🔹 모델 백본
- **UNI2-h (MahmoodLab)**  
  - 이미지 크기: 224×224  
  - Patch size: 14  
  - Embedding dimension: 1536  
  - Transformer depth: 24  
  - Head 수: 24  
  - Activation: SiLU  
  - Layer: SwiGLU packed MLP  

### 🔹 ABMIL 모델 구조

  WSI
 └── Tile Embeddings (N × 1536)
        ↓
     Feature Encoder (FC Layer)
        ↓
     Attention Module
        ├── tanh( Wv * v_i + b_v )
        └── sigmoid( W_u * v_i + b_u )
        ↓
     Weighted Aggregation (Σ α_i * v_i)
        ↓
     Classifier (FC → Softmax)
        ↓
     P(BRAF+) or P(BRAF−)


- **입력 (Input):** UNI2-h로부터 추출된 타일 임베딩 (N × 1536)  
- **Feature Encoder:** FC Layer를 통해 차원 축소 (1536 → 512)  
- **Attention Module:** 타일별 중요도 α_i 계산  
- **Aggregation:** 가중합(Σ α_i * v_i)으로 슬라이드 대표 벡터 생성  
- **Classifier:** Fully Connected + Softmax → BRAF 변이 확률 산출  



## 📈 Bag Size별 성능 비교 (WSI Instance-Level) 500장 (test 버전) || 2025-11-04 기준

| **Bag Size** | **Accuracy (mean ± std)** | **AUC (mean ± std)** | **Sensitivity** | **Specificity** | **Precision (PPV)** | **NPV** | **F1-score** |
|--------------:|----------------------------|-----------------------|-----------------|-----------------|---------------------|----------|---------------|
| **500**       | **0.840 ± 0.058**         | **0.893 ± 0.042**     | **0.880 ± 0.051** | **0.800 ± 0.131** | **0.828 ± 0.096**   | **0.872 ± 0.045** | **0.849 ± 0.046** |
| **1000**      | 0.812 ± 0.073             | 0.898 ± 0.044         | 0.848 ± 0.073     | 0.776 ± 0.128     | 0.802 ± 0.097       | 0.838 ± 0.068 | 0.820 ± 0.065 |
| **2000**      | 0.820 ± 0.052             | 0.893 ± 0.047         | 0.856 ± 0.060     | 0.784 ± 0.140     | 0.815 ± 0.098       | 0.851 ± 0.050 | 0.829 ± 0.038 |
| **3000**      | 0.808 ± 0.060             | 0.897 ± 0.048         | 0.848 ± 0.069     | 0.768 ± 0.159     | 0.805 ± 0.107       | 0.842 ± 0.056 | 0.818 ± 0.044 |
| **4000**      | 0.804 ± 0.057             | 0.896 ± 0.050         | 0.832 ± 0.069     | 0.776 ± 0.151     | 0.806 ± 0.104       | 0.830 ± 0.059 | 0.812 ± 0.043 |
| **5000**      | 0.816 ± 0.074             | 0.896 ± 0.047         | 0.840 ± 0.057     | 0.792 ± 0.155     | 0.819 ± 0.115       | 0.833 ± 0.047 | 0.824 ± 0.063 |


## example attention map

![Example Attention Heatmap](./image/example_attention_heatmap.png)
